---
title: 'An Introduction to LISA and IG-LLM'
date: 2024-05-16
permalink: /posts/2024/05/lisa/
tags:
  - Machine Learning
  - LLM
  - Graphics
---
In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, a framework that aims to connect human language and machine action. LISA is a new way of enabling machines to understand and execute complex tasks described in natural language. It has been used on navigational as well as robotic manipulation tasks and performs better than similar models in situations with limited data. 

<!-- In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, which is a framework designed connect human language and machine action. LISA is a new approach to enabling machines to understand and execute complex tasks described in natural language. LISA has been tested on navigation and robotic manipulation tasks, and it outperforms comparable models in situations with limited data. -->

In the domain of machine learning, intelligent machines (agents) are designed to solve complex tasks within their environment and find optimal solutions to unseen scenarios. The interaction between humans and agents is also a crucial part of the process. In the sequential-decision making setting, when provided expert data, an agent can learn to perform these tasks via multi-task imitation learning (IL). The task specification to an agent may happen through task ID, goal images or goal demonstrations. However, these specifications demand effort and may be hard to manage and supply during testing. Here it is highly desirable that we can specify instructions in natural language, which then the machines can carry out to solve long-horizon tasks. Therefore, the focus lies on multi-task IL setup with task-specification through language.

## Basics of Imitation Learning
Let's have a closer look at what Imitation Learning exactly is, to get a better undestanding of it before we dive into how LISA works. In reinforcement learning, an agent interacts with an environment and learns from trial and error guided by a reward function. The goal here is to learn a policy which maximizes long-term cumulative rewards. However, in some scenarios, like teaching a robot to move stuff around, coming up with a reward function can be very challanging. This is where IL comes into play. Instead of trying to learn from a reward function, IL uses a dataset of demonstrations provided by experts (humans) and tries to learn an optimal policy by imitating the experts behavior. There are two main approaches to IL: 

**Behavioral Cloning:** One of the simplest and first forms of IL is behaviour cloning (BC), which uses supervised learning to learn the expert's policy. The experts demonstrations are divided into state-action pairs. This means that the agent maps states into actions. 

**Inverse Reinforcement Learning (IRL):** The main idea of IRL is to try to learn the reward function based on the expert's demonstrations, instead of directly learing actions like in BC. After getting the reward function, reinforcement learning is used to maximize the reward function (optimal policy).

### Setup
Having a better understanding of how IL works, let's see how we can define IL formally. 
<!-- ## How Language-conditioned Skill Learning Works -->
### Language-Conditioned Skill Learning

The aim of language-conditioned IL is to solve tasks in an environment based on language-conditioned trajectories at training time and a natural language instruction at test time. This can be challenging if the task requires completing multiple sub-tasks sequentially. To make this possible, LISA uses two-level-architecture:  a skill predictor and a policy network. To explain these, let's consider the task instruction "pull the handle and move black mug right". The following graphical representation is taken from the original [paper](https://arxiv.org/abs/2203.00054):

<img src="/hayrulablog.github.io/images/lisa1.png" alt="LISA Example" align="center" width="600"/> 

**The Skill Predictor:** In the first step the instruction in natural language is analyzed and the predictor identifies the subtask needed to complete the task described. Here the language instruction is split into two independent sub-tasks "pull the handle" and "move black mug right". This is done by predicting quantized skill codes $$z$$ using a learnable codebook $$C$$ which encodes different sub-tasks.  

**The Policy Network:** The skill predictor passes the encoded sub-tasks to the policy network. Based on these the policy predicts what actions the machine should take at each time step and the encoded skill at that time step. The policy also considers the current state of the environment as well as the skill code to make decisions about the next action. 

# A Deeper Look into the Hierarchy
## Defining the Problem Setup