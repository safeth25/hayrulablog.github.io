---
title: 'An Introduction to LISA and IG-LLM'
date: 2024-05-16
permalink: /posts/2024/05/lisa/
tags:
  - Machine Learning
  - LLMs
  - Graphics
---
In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, a framework that aims to connect human language and machine action. LISA is a new way of enabling machines to understand and execute complex tasks described in natural language. It has been used on navigational as well as robotic manipulation tasks and performs better than similar models in situations with limited data. 

In the domain of machine learning, intelligent machines (agents) are designed to solve complex tasks within their environment and find optimal solutions to unseen scenarios. The interaction between humans and agents is also a crucial part of the process. In the sequential-decision making setting, when provided expert data, an agent can learn to perform these tasks via multi-task imitation learning (IL). The task specification to an agent may happen through task ID, goal images or goal demonstrations. However, these specifications demand effort and may be hard to manage and supply during testing. Here it is highly desirable that we can specify instructions in natural language, which then the machines can carry out to solve long-horizon tasks. Therefore, the focus lies on multi-task IL setup with task-specification through language.

## Basics of Imitation Learning
Let's have a closer look at what Imitation Learning exactly is, to get a better undestanding of it before we dive into how LISA works. In reinforcement learning, an agent interacts with an environment and learns from trial and error guided by a reward function. The goal here is to learn a policy which maximizes long-term cumulative rewards. However, in some scenarios, like teaching a robot to move stuff around, coming up with a reward function can be very challanging. This is where IL comes into play. Instead of trying to learn from a reward function, IL uses a dataset of demonstrations provided by experts (humans) and tries to learn an optimal policy by imitating the experts behavior. There are two main approaches to IL: 

**Behavioral Cloning:** One of the simplest and first forms of IL is behaviour cloning (BC), which uses supervised learning to learn the expert's policy. The experts demonstrations are divided into state-action pairs. This means that the agent maps states into actions. 

**Inverse Reinforcement Learning (IRL):** The main idea of IRL is to try to learn the reward function based on the expert's demonstrations, instead of directly learing actions like in BC. After getting the reward function, reinforcement learning is used to maximize the reward function (optimal policy).

### Defining the Environment
Having a better understanding of how IL works, let's see how we can define IL formally. In IL the environment can be represented as a Markov Decision Process (MDP). The environment in MDP consists a set of States $$\mathcal{S}$$, a set of Actions $$\mathcal{A}$$, a transition model $$P(s'|s, a)$$ which represents the probability of that an action $$a$$ in the state $$s$$ leads to state $$s’$$ and a reward function $$R(s,a)$$ that has to be learned. The agent then acts in this environment based on the policy &pi;. 

Since we focus on language-conditioned skill learning, we augment the MDP with a set of different tasks $$\mathcal{T}$$. A Task can be constructed from multiple sub-tasks $$\mathcal{T}_i$$ in no particular order. Every Task also has a description in natural language $$l \in L$$, where $$L$$ is the space of language instructions. The expert's dataset is represented as $$\mathcal{D} = \{\tau_1, \tau_2, \ldots, \tau_m \}$$. An expert's demonstration is called a trajectory $$\tau^i = (l^i, \{(s_1^i, a_1^i), (s_2^i, a_2^i), \ldots, (s_T^i, a_T^i)\})$$ . The trajectory consists of observations $$s_t^i \in \mathcal{S}$$ and actions $$a_t^i \in \mathcal{A}$$ over $$T$$ timesteps. The goal is to predict the experts action $$a_t$$ given past observations and a natural language isntruction.
<!-- ## How Language-conditioned Skill Learning Works -->
## A Deeper Look into the Hierarchy
The aim of language-conditioned IL is to solve tasks in an environment based on language-conditioned trajectories at training time and a natural language instruction at test time. This can be challenging if the task requires completing multiple sub-tasks sequentially. To make this possible, LISA uses two-level-architecture:  a skill predictor and a policy network. 

<!-- ### Language-Conditioned Skill Learning -->
To explain these, let's consider the task instruction "pull the handle and move black mug right". The following graphical representation is taken from the original [paper](https://arxiv.org/abs/2203.00054):

<div align="center">
<img src="/hayrulablog.github.io/images/lisa1.png" alt="LISA Example" align="center" width="600"/> 
</div>
**The Skill Predictor:** In the first step the instruction in natural language is analyzed and the predictor identifies the subtask needed to complete the task described. Here the language instruction is split into two independent sub-tasks "pull the handle" and "move black mug right". This is done by predicting quantized skill codes $$z$$ using a learnable codebook $$\mathcal{C}$$ which encodes different sub-tasks. 

**The Policy Network:** The skill predictor passes the encoded sub-tasks to the policy network, defined as  &pi; $$: \mathcal{S} \times \mathcal{C} \rightarrow \mathcal{A}$$. Based on these the policy predicts what actions the machine should take at each time step and the encoded skill at that time step. The policy also considers the current state of the environment as well as the skill code to make decisions about the next action. 

Here is a graphic of the LISA architecture: 

<div align="center">
<img src="/hayrulablog.github.io/images/lisa2.png" alt="LISA Example" align="center" width="400"/> 
</div>
*Figure sourced from the [original LISA paper](https://arxiv.org/abs/2203.00054).*

As seen in the graphic above the skill predictor $$f$$, which is a function defined as: 
<div align="center">
$$ f: L \times \mathcal{S} \rightarrow \mathcal{C}$$
</div>
becomes an input of language instruction and a sequence of observations $$ \tau = (l, \{s_t, a_t\}^T_{t = 1})$$. The predictor generates a continuous skill embedding $$\widetilde{z} \in \mathcal{R}^{\mathcal{D}}$$, where: 
<div align="center">
 $$\widetilde{z} = f(l, (s_t, s_{t-1}, \ldots))$$
</div>
Vector Quantization (VQ) is then used to map this continuous embedding $$\tilde{z}$$ to a discrete representation: 

<div align="center">
$$  z = q(\widetilde{z}) =: \text{arg min}_{z^k \in \mathcal{C}} \Vert \widetilde{z} - z^k \Vert_2 $$
</div>

Here, $$ \mathcal{C} = \{z^1, \ldots, z^K\} $$ represents the Codebook containing $$K$$ discrete vectors, each representing a unique skill learned from the data. The goal is to map the continuous embedding $$\tilde{z}$$ to its nearest vector $$z$$ in the codebook, similar to $$k$$-means clustering.

Since the quantization operation is not differentiable, traditional gradient-based optimization methods are difficult to apply. LISA uses a straight-through gradient estimator to address this, which copies gradients directly from the decoder to the encoder during training, allowing learning of discrete skill representations.

The policy $$\pi$$ uses the chosen skill code $$z$$ and the sequence of observations to determine the actions. The skill code $$z$$ is persisted for $$H$$ timesteps (the horizon), after which the skill predictor is invoked again to predict a new skill.

**LISA's Training Objective:** LISA is trained end-to-end using an objective function designed to optimize both the behavior-cloning policy and the skill predictor. The objective, $$L_{\text{LISA}}$$, is composed of two components: the behavior-cloning loss $$L_{\text{BC}}$$ and the vector quantization loss $$L_{\text{VQ}}$$, combined as follow: 
<div align="center">
$$L_{\text{LISA}} = L_{\text{BC}} + \lambda L_{\text{VQ}}$$
</div>
Here, $$L_{\text{BC}}$$ represents the behavior-cloning loss on the policy $$\pi_{\theta}$$, and $$\lambda$$ is a weighting factor for the vector quantization loss $$L_{\text{VQ}}$$. 

The vector quantization loss $$L_{\text{VQ}}$$ is applied to the skill predictor $$f_{\phi}$$ and is defined as: 
<div align="center">
$$L_{\text{VQ}}(f) = \mathbb{E}[\Vert\text{sg}[q(\tilde{z})] - \tilde{z}\Vert^2]$$
</div>

The term $$\text{sg}[\cdot]$$ denotes the stop-gradient operation, which stops gradients from flowing back through the quantization process.


Below is the pseudocode for the LISA training algorithm:
<div align="center">
<img src="/hayrulablog.github.io/images/algo.png" alt="LISA Training Algorithm" align="center" width="600"/> 
</div>

*Algorithm sourced from the [original LISA paper](https://arxiv.org/abs/2203.00054).*

## Experiments
In this section, we look how LISA performs on various tasks and compare it with other methods. We evaluate LISA's capabilities on grid-world navigation and robotic manipulation tasks, comparing it against a strong non-hierarchical baseline in low-data scenarios.

To put LISA to the test, two distinct datasets were used: 

**BabyAI Dataset:** The BabyAI dataset contains 19 levels of increasing difficulty in a 7x7 grid world with multiple rooms. The agent has to perform different tasks like moving objects between rooms, opening or closing doors with only partially observed state and a language instruction. For each level the dataset provides 1 million language conditioned trajectories, but only 0.1% to 10% of these trajectories were utilized to train the model. The evaluation involves testing on 100 different, randomly generated instructions for each level.

<div align="center">
<img src="/hayrulablog.github.io/images/babyAIDATA.png" alt="BabyAI BossLevel" align="center" width="400"/> 
<p align="center" width="200">Boss Level example command: "pick up the grey box behind you, then go to the grey key and open a door."</p>
</div>

*Figure sourced from the [original BabyAI paper](https://arxiv.org/abs/1810.08272).*

**LOReL Sawyer Dataset:** Next is the LORel Sawyer dataset, which includes 50,000 pseudo-expert trajectories collected from a replay buffer of a random reinforcment learning (RL) policy. Furthermore, these trajectories are annotated with crowd-sourced language instructions.

<div align="center">
<img src="/hayrulablog.github.io/images/LOReL.png" alt="LOReL Dataset" align="center" width="400"/> 
<p align="center" width="800">Language-conditioned Offline Reward Learning (LORel)</p>
</div>
*Figure sourced from the [original LOReL paper](https://arxiv.org/abs/2109.01115).*

LISA was evaluated on the same set of six tasks used in the original paper:

- closing a drawer
- opening a drawer
- turning a handle right
- turning a handle left
- moving a black mug right
- moving a white mug down

Each task was varied by changing nouns, verbs, or the entire instruction to test different aspects of the model's performance, with a total of 77 variations of these tasks. The evaluation was conducted in two different settings: using full robot state space observations and with partially observed images.

To benchmark LISA's performance, several baseline approaches were used:

**Original Baselines:** For the BabyAI dataset LISA was comparend against a non-hierarchical recurrent neural network (RNN) based method from the original paper. Similarly, on the LORel dataset, LISA was evaluated against a language-conditioned behavior cloning (BC) method. While the LOReL method initially uses a planning algorithm with a learned reward function to address sub-optimal trajectories, for LISA a BC baseline is more equitable comparison since it also employs BC.

**Flat Baseline:** This non-hierarchical baseline is based on a language-conditioned Decision Transformer (DT) and is similar to LISA in implementation but lacks a skill predictor network. The policy here is a Causal Transformer, which uses a numerical representation of the language instructions provided by a pre-trained DistillBERT model, instead of the future sum of rewards to guide its actions.

# Performance Comparison in Low-Data Regime

LISA's performance was assessed on three levels from the BabyAI environment (GoToSeq, SynthSeq and BossLevel)  and the LORel Sawyer environment. These levels were chosen because of their complexity and requirement to perform multiple sub-tasks in sequence. 

The experimental results are presented in the Table below:

<div align="center">
<img src="/hayrulablog.github.io/images/results.png" alt="LISA Training Algorithm" align="center" width="600"/> 
</div>

*Table sourced from the [original LISA paper](https://arxiv.org/abs/2203.00054).*