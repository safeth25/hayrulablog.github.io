---
title: 'An Introduction to LISA and IG-LLM'
date: 2024-05-16
permalink: /posts/2024/05/lisa/
tags:
  - Machine Learning
  - LLM
  - Graphics
---
In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, a framework that aims to connect human language and machine action. LISA is a new way of enabling machines to understand and execute complex tasks described in natural language. It has been used on navigational as well as robotic manipulation tasks and performs better than similar models in situations with limited data. 

<!-- In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, which is a framework designed connect human language and machine action. LISA is a new approach to enabling machines to understand and execute complex tasks described in natural language. LISA has been tested on navigation and robotic manipulation tasks, and it outperforms comparable models in situations with limited data. -->

In the domain of machine learning, intelligent machines (agents) are designed to solve complex tasks within their environment and find optimal solutions to unseen scenarios. The interaction between humans and agents is also a crucial part of the process. In the sequential-decision making setting, when provided expert data, an agent can learn to perform these tasks via multi-task imitation learning (IL). The task specification to an agent may happen through task ID, goal images or goal demonstrations. However, these specifications demand effort and may be hard to manage and supply during testing. Here it is highly desirable that we can specify instructions in natural language, which then the machines can carry out to solve long-horizon tasks. Therefore, the focus lies on multi-task IL setup with task-specification through language.

## Basics of Imitation Learning
Let's have a closer look at what Imitation Learning exactly is, to get a better undestanding of it before we dive into how LISA works. In reinforcement learning, an agent interacts with an environment and learns from trial and error guided by a reward function. The goal here is to learn a policy which maximizes long-term cumulative rewards. However, in some scenarios, like teaching a robot to move stuff around, coming up with a reward function can be very challanging. This is where IL comes into play. Instead of trying to learn from a reward function, IL uses a dataset of demonstrations provided by experts (humans) and tries to learn an optimal policy by imitating the experts behavior. There are two main approaches to IL: 

**Behavioral Cloning:** One of the simplest and first forms of IL is behaviour cloning (BC), which uses supervised learning to learn the expert's policy. The experts demonstrations are divided into state-action pairs. This means that the agent maps states into actions. 

**Inverse Reinforcement Learning (IRL):** The main idea of IRL is to try to learn the reward function based on the expert's demonstrations, instead of directly learing actions like in BC. After getting the reward function, reinforcement learning is used to maximize the reward function (optimal policy).

### Defining the Environment
Having a better understanding of how IL works, let's see how we can define IL formally. In IL the environment is represented as a Markov Decision Process (MDP). The environment in MDP consists a set of States $$\mathcal{S}$$, a set of Actions $$\mathcal{A}$$, a transition model $$P(s'|s, a)$$ which represents the probability of that an action a in the state s leads to state sâ€™ and a reward function $$R(s,a)$$ that has to be learned. The agent then acts in this environment based on the policy &pi;. 

Since we focus on language-conditioned skill learning, we augment the MDP with a set of different tasks $$\mathcal{T}$$. A Task can be constructed from multiple sub-tasks $$\mathcal{T}_i$$ in no particular order. Every Task also has a description in natural language $$l \in L$$, where $$L$$ is the space of language instructions. The expert's dataset is represented as $$\mathcal{D}$$. An expert's demonstration is called a trajectory $$\tau^i = (l^i, \{(s_1^i, a_1^i), (s_2^i, a_2^i), \ldots, (s_T^i, a_T^i)\})$$ . The trajectory consists of observations $$s_t^i \in \mathcal{S}$$ and actions $$a_t^i \in \mathcal{A}$$ over $$T$$ timesteps. The goal is to predict the experts action $$a_t$$ given past observations and a natural language isntruction.
<!-- ## How Language-conditioned Skill Learning Works -->
## A Deeper Look into the Hierarchy
The aim of language-conditioned IL is to solve tasks in an environment based on language-conditioned trajectories at training time and a natural language instruction at test time. This can be challenging if the task requires completing multiple sub-tasks sequentially. To make this possible, LISA uses two-level-architecture:  a skill predictor and a policy network. 

<!-- ### Language-Conditioned Skill Learning -->
To explain these, let's consider the task instruction "pull the handle and move black mug right". The following graphical representation is taken from the original [paper](https://arxiv.org/abs/2203.00054):

<img src="/hayrulablog.github.io/images/lisa1.png" alt="LISA Example" align="center" width="600"/> 

**The Skill Predictor:** In the first step the instruction in natural language is analyzed and the predictor identifies the subtask needed to complete the task described. Here the language instruction is split into two independent sub-tasks "pull the handle" and "move black mug right". This is done by predicting quantized skill codes $$z$$ using a learnable codebook $$C$$ which encodes different sub-tasks. The skill predictor is a function defines as: $$ f: L \times \mathcal{S} \rightarrow \mathcal{C}$$, where $$\mathcal{C} = \{z^1, \ldots, z^K\}$$ is a Codebook of $$K$$ quantized skills.

**The Policy Network:** The skill predictor passes the encoded sub-tasks to the policy network, defines as  &pi; $$: \mathcal{S} \times \mathcal{C} \rightarrow \mathcal{A}$$. Based on these the policy predicts what actions the machine should take at each time step and the encoded skill at that time step. The policy also considers the current state of the environment as well as the skill code to make decisions about the next action. 

Here is a Diagramm of the LISA architecture taken from the original paper: 
<img src="/hayrulablog.github.io/images/lisa2.png" alt="LISA Example" align="center" width="400"/> 

<!-- BLOG POST -->
# Re-Thinking Inverse Graphics With Large Language Models
Inverse Graphics (IG) is the process of disentangling an image into physical variables that contain information about its constituent properties such as its shape, color, material, position, rotation etc. This task is often equated with "analysis by synthesis" and is needed to recreate scenes that are only available as single 2D/3D images. 
The biggest problem with IG is that it is very difficult to generalize features such as comprehensive understanding of a 3D environment's spatial and physical properties. This limits their ability to generalize hugely. In the recent past, different approaches to solve this problem has ben researched. This research's goal is to find a new approach to solving this problem, thus creating graphics programs that are able to reproduce the scenes using a traditional graphics engine.

The authors are planning to leverage the proficiency of Large Language Models (LLMs) as LLMs exhibit compositional reasoning abilities that could be beneficial for spatial reasoning in 3D tasks. In order to achieve this, they propose the Inverse-Graphics Large Language Model (IG-LLM): an inverse graphics framework centered around an LLM, that transforms a visual embedding into a structured, compositional 3D-scene representation autoregressively. This process works beyond mere pixel- or object-level interpretation of an image and tries to find physical relationships among essential objects of the scene.

<img src="/images/ig-llm-blenderpipe.png" alt="IG-LLM Example" align="center" width="600"/>

<h3 id="why llms">Why Large Language Models?</h3>

To understand what IG-LLM does differently than other approaches that try to develop inverse graphics methods, we must first know what progress was made before IG-LLM. Though there are multiple approaches, they all have some features in common, such as analysing every object in the scene indepentently, completely overlooking the semantics and interrelation between objects, hindering any reasoning. And the most common problem they all have is that they all have generalization problems because of the way they were constructed. That means they have to rely on task-spesific inductive biases or detailed and spesific training sets.

This is where LLMs come into play. One of the greatest successes of LLMs is their exceptional ability to generalize. Unlike traditional approaches, LLMs are primarily trained via and unsupervised next-token-prediction, thereby unifying various natural language processing (NLP) tasks within a generic framework and not needing to scale the amount of task-spesific training data.

In this research, through the use of an incorporated frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training, IG-LLM solves the IG problem through next-token prediction without needing to use image-space supervision, thus opening up new possibilities to exploit the visual knowledge of LLMs for solving IG problems. 

Tuning LLMs for Inverse Graphics
------

While LLMs create complete language-token sequences, past visual question answering (VQA) works have shown that large pretrained vision transformers can also be used as visual tokenizers. By fusing linguistic tokens for the LLM with visual embeddings, these studies integrate the understanding of images and words. They take an approach that is consistent with this method, building an LLM that can "see" the input image and provide a structured code representation of the input scene.

<h3 id="architecture">Architecture of the IG-LLM</h3>

The IG-LLM model is based on an instruction-tuned variant of LLaMA-1 7B in conjunction with a frozen CLIP vision encoder, used as the visual tokenizer. 

<h3 id="training">Training the IG-LLM / Vision-Language Alignment</h3>

The linear vision-encoder projection is initially trained using the feature-alignment pre-training method from LLaVA. Instruction sequences created from image-caption pairings taken from the Conceptual Captions dataset are used in this training. During training, the language model (LLM) receives an image embedding produced by the vision encoder, along with a randomly selected prompt that instructs the model to describe both the image and its caption. Throughout this pre-training stage, all parameters of the model remain unchanged, except for the learnable vision projector.


<h3 id="trainingdata">Training Data Generation</h3>

The procedurally created dataset CLEVR consists of basic three-dimensional objects arranged on a plane. The shape (such as spheres, cubes, etc.), size, color, and spatial pose of these objects, also known as primitives, are all randomly chosen. Shape, size, color, and material are discrete qualities; on the other hand, the pose, which indicates the object's location and orientation, is a continuous property. The images from the sampled scenes are rendered using the Blender modeling software from its Python scripting API.

<img src="/images/cleversample.png" alt="clevrsample" align="center" width="600"/>

To train the model, pairs of codes and their corresponding images are given to the model, and the model is tasked with figuring what code could be used to reproduce the image in the given pair. After that, the model is continued to train with a traditional next-token prediction task, with the goal of maximizing the conditional probability of the next token.

<img src="/images/pifalanfilan.png" alt="formula1" align="center" width="200"/>

Where s_i represents the ith token.

<img src="/images/numerichead.png" alt="numerichead" align="center" width="600"/>

As you can observe in the image above, the "Numeric Head" concept is introduced to handle numbers differently in this model. Instead of having the model generate numbers as individual digits, it has been teached to produce a special [NUM] token whenever a number is needed. This [NUM] token acts as a placeholder, indicating that the number should be processed separately.

<h3 id="differences">Differences from Traditional Approaches</h3>

The framework introduced in this model differs from the traditional approaches in a big way. The visual-encoding process of this new framework does not work with graphics-spesific inductive biases. It does not go through any training for intermediate vision tasks, such as recognizing objects, segmentation or attribute regression. The model does not have access to the 3D models and thus only works with the rendered images. Although this approach might look less accurate at first, with further testing it is seen that this approach increases the shape-recognition accuracy by approximately 60% compared to the conventional ways.

<h3 id="numericreasoning">Precise Numeric Reasoning in LLMs</h3>

//TODO

Evaluations
------

//SIDENOTE: All of the text in this draft will be reconsidered and hopefully will be written better. This is just a draft.

//TODO: From this part on, I am planning to show the tests and their results in comparison with more conventional approaches, I will use the images at the end of the paper to talk about the differences in the results, and then write about my own opinions on this whole project. My teammate Safet and I will try to connect our blog posts to each other and write about the general problems, future directions, etc. of projects like LISA and IG-LLM.

<h3 id="compgenerclevr">Compositional Generarlization on CLEVR</h3>

<h3 id="numericparameterspace">Numeric Parameter-Space Generalization</h3>

<h3 id="6dofpose">6-DoF Pose Estimation</h3>

Conclusion
------