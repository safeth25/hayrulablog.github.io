---
title: 'An Introduction to LISA and IG-LLM'
date: 2024-05-16
permalink: /posts/2024/05/lisa/
tags:
  - Machine Learning
  - LLMs
  - Graphics
---
In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, a framework that aims to connect human language and machine action. LISA is a new way of enabling machines to understand and execute complex tasks described in natural language. It has been used on navigational as well as robotic manipulation tasks and performs better than similar models in situations with limited data. 

<!-- In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, which is a framework designed connect human language and machine action. LISA is a new approach to enabling machines to understand and execute complex tasks described in natural language. LISA has been tested on navigation and robotic manipulation tasks, and it outperforms comparable models in situations with limited data. -->

In the domain of machine learning, intelligent machines (agents) are designed to solve complex tasks within their environment and find optimal solutions to unseen scenarios. The interaction between humans and agents is also a crucial part of the process. In the sequential-decision making setting, when provided expert data, an agent can learn to perform these tasks via multi-task imitation learning (IL). The task specification to an agent may happen through task ID, goal images or goal demonstrations. However, these specifications demand effort and may be hard to manage and supply during testing. Here it is highly desirable that we can specify instructions in natural language, which then the machines can carry out to solve long-horizon tasks. Therefore, the focus lies on multi-task IL setup with task-specification through language.

## Basics of Imitation Learning
Let's have a closer look at what Imitation Learning exactly is, to get a better undestanding of it before we dive into how LISA works. In reinforcement learning, an agent interacts with an environment and learns from trial and error guided by a reward function. The goal here is to learn a policy which maximizes long-term cumulative rewards. However, in some scenarios, like teaching a robot to move stuff around, coming up with a reward function can be very challanging. This is where IL comes into play. Instead of trying to learn from a reward function, IL uses a dataset of demonstrations provided by experts (humans) and tries to learn an optimal policy by imitating the experts behavior. There are two main approaches to IL: 

**Behavioral Cloning:** One of the simplest and first forms of IL is behaviour cloning (BC), which uses supervised learning to learn the expert's policy. The experts demonstrations are divided into state-action pairs. This means that the agent maps states into actions. 

**Inverse Reinforcement Learning (IRL):** The main idea of IRL is to try to learn the reward function based on the expert's demonstrations, instead of directly learing actions like in BC. After getting the reward function, reinforcement learning is used to maximize the reward function (optimal policy).

### Defining the Environment
Having a better understanding of how IL works, let's see how we can define IL formally. In IL the environment is represented as a Markov Decision Process (MDP). The environment in MDP consists a set of States $$\mathcal{S}$$, a set of Actions $$\mathcal{A}$$, a transition model $$P(s'|s, a)$$ which represents the probability of that an action a in the state s leads to state s’ and a reward function $$R(s,a)$$ that has to be learned. The agent then acts in this environment based on the policy &pi;. 

Since we focus on language-conditioned skill learning, we augment the MDP with a set of different tasks $$\mathcal{T}$$. A Task can be constructed from multiple sub-tasks $$\mathcal{T}_i$$ in no particular order. Every Task also has a description in natural language $$l \in L$$, where $$L$$ is the space of language instructions. The expert's dataset is represented as $$\mathcal{D} = \{\tau_1, \tau_2, \ldots, \tau_m \}$$. An expert's demonstration is called a trajectory $$\tau^i = (l^i, \{(s_1^i, a_1^i), (s_2^i, a_2^i), \ldots, (s_T^i, a_T^i)\})$$ . The trajectory consists of observations $$s_t^i \in \mathcal{S}$$ and actions $$a_t^i \in \mathcal{A}$$ over $$T$$ timesteps. The goal is to predict the experts action $$a_t$$ given past observations and a natural language isntruction.
<!-- ## How Language-conditioned Skill Learning Works -->
## A Deeper Look into the Hierarchy
The aim of language-conditioned IL is to solve tasks in an environment based on language-conditioned trajectories at training time and a natural language instruction at test time. This can be challenging if the task requires completing multiple sub-tasks sequentially. To make this possible, LISA uses two-level-architecture:  a skill predictor and a policy network. 

<!-- ### Language-Conditioned Skill Learning -->
To explain these, let's consider the task instruction "pull the handle and move black mug right". The following graphical representation is taken from the original [paper](https://arxiv.org/abs/2203.00054):

<div align="center">
<img src="/hayrulablog.github.io/images/lisa1.png" alt="LISA Example" align="center" width="600"/> 
</div>
**The Skill Predictor:** In the first step the instruction in natural language is analyzed and the predictor identifies the subtask needed to complete the task described. Here the language instruction is split into two independent sub-tasks "pull the handle" and "move black mug right". This is done by predicting quantized skill codes $$z$$ using a learnable codebook $$\mathcal{C}$$ which encodes different sub-tasks. The skill predictor is a function defined as: $$ f: L \times \mathcal{S} \rightarrow \mathcal{C}$$, where $$\mathcal{C} = \{z^1, \ldots, z^K\}$$ is a Codebook of $$K$$ quantized skills.

**The Policy Network:** The skill predictor passes the encoded sub-tasks to the policy network, defined as  &pi; $$: \mathcal{S} \times \mathcal{C} \rightarrow \mathcal{A}$$. Based on these the policy predicts what actions the machine should take at each time step and the encoded skill at that time step. The policy also considers the current state of the environment as well as the skill code to make decisions about the next action. 

Here is a Graphic of the LISA architecture taken from the original paper: 

<div align="center">
<img src="/hayrulablog.github.io/images/lisa2.png" alt="LISA Example" align="center" width="400"/> 
</div>
As seen in the Graphic above the skill predictor $$f$$ becomes an input of language instruction and a sequence of observations $$ \tau = (l, \{s_t, a_t\}^T_{t = 1})$$. The predictor generates a continuous skill embedding $$\widetilde{z} \in \mathcal{R}^{\mathcal{D}}$$, where $$\widetilde{z} = f(l, (s_t, s_{t-1}, \ldots))$$.

Vector Quantization (VQ) is then used to map this continuous embedding $$\tilde{z}$$ to a discrete representation: 

<div align="center">
$$  z = q(\widetilde{z}) =: \text{arg min}_{z^k \in \mathcal{C}} \Vert \widetilde{z} - z^k \Vert_2 $$
</div>

Here, $$ \mathcal{C} = \{z^1, \ldots, z^K\} $$ represents the Codebook containing $$K$$ discrete vectors, each representing a unique skill learned from the data.

**Vector Quantization: Discretizing Skills**

## LISA's Training Objective
