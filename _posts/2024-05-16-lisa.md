---
title: 'A New Approach to Language-Conditioned Tasks with LISA'
date: 2024-05-16
permalink: /posts/2024/05/lisa/
tags:
  - Machine Learning
  - LLMs
  - Graphics
---

In this blog post, we introduce LISA: Learning Interpretable Skill Abstractions from Language, a framework that aims to connect human language and machine action. LISA is a new way of enabling machines to understand and execute complex tasks described in natural language. It has been used on navigational as well as robotic manipulation tasks and performs better than similar models in situations with limited data. 
## Table of Content
- [Basics of Imitation Learning](#basicsOfIL)
- [LISA Architecture](#architecture)
- [Experiments](#experiments)
- [Evaluation](#evaluation)
- [Conclusion](#conclusion)

## Basics of Imitation Learning {#basicsOfIL}
In the domain of machine learning, intelligent machines (agents) are designed to solve complex tasks within their environment and find optimal solutions to unseen scenarios. The interaction between humans and agents is also a crucial part of the process. In the sequential-decision making setting, when provided expert data, an agent can learn to perform these tasks via multi-task imitation learning (IL). The task specification to an agent may happen through task ID <a href="#reference-8">[8]</a>, goal images or goal demonstrations. However, these specifications demand effort and may be hard to manage and supply during testing. Here it is highly desirable that we can specify instructions in natural language, which then the machines can carry out to solve long-horizon tasks. Therefore, the focus lies on multi-task IL setup with task-specification through language.

Imitation Learning (IL) helps agents learn from expert demonstrations instead of a reward function. This is useful when creating a reward function is challenging, such as teaching a robot to move objects. IL has two main approaches:


**Behavioral Cloning:** Uses supervised learning to map states to actions based on expert demonstrations.

**Inverse Reinforcement Learning (IRL):** Learns the reward function from expert demonstrations and then uses reinforcement learning to maximize this reward.

### Defining the Environment
IL can be represented as a Markov Decision Process (MDP) with:
- States ($$\mathcal{S}$$)
- Actions ($$\mathcal{A}$$)
- Transition model ($$P(s' \vert s, a)$$)
- Reward function ($$R(s,a)$$)

The agent's goal is to act based on the policy (&pi;).

For language-conditioned skill learning, we augment the MDP with tasks ($$\mathcal{T}$$) described in natural language ($$l \in L$$). The expert's dataset is represented as $$\mathcal{D} = \{\tau_1, \tau_2, \ldots, \tau_m \}$$. A trajectory ($$\tau^i$$) consists of observations ($$s_t^i$$) and actions ($$a_t^i$$) over $$T$$ timesteps. The goal is to predict the expert's action ($$a_t$$) given past observations and a natural language instruction. 

<!-- ## How Language-conditioned Skill Learning Works -->
## A Deeper Look into the Hierarchy {#architecture}
The aim of language-conditioned IL is to solve tasks in an environment based on language-conditioned trajectories at training time and a natural language instruction at test time. This can be challenging if the task requires completing multiple sub-tasks sequentially. To make this possible, LISA uses two-level-architecture:  a skill predictor and a policy network. 

<!-- ### Language-Conditioned Skill Learning -->
To explain these, let's consider the task instruction "pull the handle and move black mug right". 

<div align="center">
<img src="/hayrulablog.github.io/images/lisa1.png" alt="LISA Example" align="center" width="600"/> 
<p style="font-size= 14px;">Figure taken from <a href="#reference-1">[1]</a></p>
</div>
**The Skill Predictor:** In the first step the instruction in natural language is analyzed and the predictor identifies the subtask needed to complete the task described. Here the language instruction is split into two independent sub-tasks "pull the handle" and "move black mug right". This is done by predicting quantized skill codes $$z$$ using a learnable codebook $$\mathcal{C}$$ which encodes different sub-tasks. 

**The Policy Network:** The skill predictor passes the encoded sub-tasks to the policy network, defined as  &pi; $$: \mathcal{S} \times \mathcal{C} \rightarrow \mathcal{A}$$. Based on these the policy predicts what actions the machine should take at each time step and the encoded skill at that time step. The policy also considers the current state of the environment as well as the skill code to make decisions about the next action. 

Here is a graphic of the LISA architecture: 

<div align="center">
<img src="/hayrulablog.github.io/images/lisa2.png" alt="LISA Example" align="center" width="400"/> 
<p style="font-size=14px;">Figure taken from <a href="#reference-1">[1]</a></p>
</div>

As seen in the graphic above the skill predictor $$f$$, which is a function defined as: 
<div align="center">
$$ f: L \times \mathcal{S} \rightarrow \mathcal{C}$$
</div>
becomes an input of language instruction and a sequence of observations $$ \tau = (l, \{s_t, a_t\}^T_{t = 1})$$. The predictor generates a continuous skill embedding $$\widetilde{z} \in \mathcal{R}^{\mathcal{D}}$$, where: 
<div align="center">
 $$\widetilde{z} = f(l, (s_t, s_{t-1}, \ldots))$$
</div>
Vector Quantization (VQ) <a href="#references-5">[5]</a> is then used to map this continuous embedding $$\tilde{z}$$ to a discrete representation: 

<div align="center">
$$  z = q(\widetilde{z}) =: \text{arg min}_{z^k \in \mathcal{C}} \VertÂ \widetilde{z} - z^k \Vert_2 $$
</div>

Here, $$ \mathcal{C} = \{z^1, \ldots, z^K\} $$ represents the Codebook containing $$K$$ discrete vectors, each representing a unique skill learned from the data. The goal is to map the continuous embedding $$\tilde{z}$$ to its nearest vector $$z$$ in the codebook, similar to $$k$$-means clustering <a href="#reference-11">[11]</a>.

Since the quantization operation is not differentiable, traditional gradient-based optimization methods are difficult to apply. LISA uses a straight-through gradient estimator to address this, which copies gradients directly from the decoder to the encoder during training, allowing learning of discrete skill representations.

The policy $$\pi$$ uses the chosen skill code $$z$$ and the sequence of observations to determine the actions. The skill code $$z$$ is persisted for $$H$$ timesteps (the horizon), after which the skill predictor is invoked again to predict a new skill.

**LISA's Training Objective:** LISA is trained end-to-end using an objective function designed to optimize both the behavior-cloning policy and the skill predictor. The objective, $$L_{\text{LISA}}$$, is composed of two components: the behavior-cloning loss $$L_{\text{BC}}$$ and the vector quantization loss $$L_{\text{VQ}}$$, combined as follow: 
<div align="center">
$$L_{\text{LISA}} = L_{\text{BC}} + \lambda L_{\text{VQ}}$$
</div>
Here, $$L_{\text{BC}}$$ represents the behavior-cloning loss on the policy $$\pi_{\theta}$$, and $$\lambda$$ is a weighting factor for the vector quantization loss $$L_{\text{VQ}}$$. 

The vector quantization loss $$L_{\text{VQ}}$$ is applied to the skill predictor $$f_{\phi}$$ and is defined as: 
<div align="center">
$$L_{\text{VQ}}(f) = \mathbb{E}[\Vert\text{sg}[q(\tilde{z})] - \tilde{z}\Vert^2]$$
</div>

The term $$\text{sg}[\cdot]$$ denotes the stop-gradient operation, which stops gradients from flowing back through the quantization process.


Below is the pseudocode for the LISA training algorithm:
<div align="center">
<img src="/hayrulablog.github.io/images/algo.png" alt="LISA Training Algorithm" align="center" width="600"/> 
<p style="font-size=14px;">Algorithm taken from <a href="#reference-1">[1]</a></p>
</div>

*Algorithm sourced from the [original LISA paper](https://arxiv.org/abs/2203.00054).*

## Experiments {#experiments}
In this section, we look how LISA performs on various tasks and compare it with other methods. We evaluate LISA's capabilities on grid-world navigation and robotic manipulation tasks, comparing it against a strong non-hierarchical baseline in low-data scenarios.

To put LISA to the test, two distinct datasets were used: 

**BabyAI Dataset:** The BabyAI dataset contains 19 levels of increasing difficulty in a 7x7 grid world with multiple rooms. The agent has to perform different tasks like moving objects between rooms, opening or closing doors with only partially observed state and a language instruction. For each level the dataset provides 1 million language conditioned trajectories, but only 0.1% to 10% of these trajectories were utilized to train the model. The evaluation involves testing on 100 different, randomly generated instructions for each level.

<div align="center">
<img src="/hayrulablog.github.io/images/babyAIDATA.png" alt="BabyAI BossLevel" align="center" width="400"/> 
<p style="font-size:14px;">Boss Level example command: "pick up the grey box behind you, then go to the grey key and open a door." <br> Figure taken from <a href="#reference-4">[4]</a></p>
</div>

**LOReL Sawyer Dataset:** Next is the LORel Sawyer dataset, which includes 50,000 pseudo-expert trajectories collected from a replay buffer of a random reinforcment learning (RL) policy. Furthermore, these trajectories are annotated with crowd-sourced language instructions.

<div align="center">
<img src="/hayrulablog.github.io/images/LOReL.png" alt="LOReL Dataset" align="center" width="400"/> 
<p style="font-size:14px;">Language-conditioned Offline Reward Learning (LORel) <br> Figure taken from <a href="#reference-7">[7]</a></p>
</div>

LISA was evaluated on the same set of six tasks used in the original paper:

- closing a drawer
- opening a drawer
- turning a handle right
- turning a handle left
- moving a black mug right
- moving a white mug down

Each task was varied by changing nouns, verbs, or the entire instruction to test different aspects of the model's performance, with a total of 77 variations of these tasks. The evaluation was conducted in two different settings: using full robot state space observations and with partially observed images.

To benchmark LISA's performance, several baseline approaches were used:

**Original Baselines:** For the BabyAI dataset LISA was comparend against a non-hierarchical recurrent neural network (RNN) based method from the original paper. Similarly, on the LORel dataset, LISA was evaluated against a language-conditioned behavior cloning (BC) method. While the LOReL method initially uses a planning algorithm with a learned reward function to address sub-optimal trajectories, for LISA a BC baseline is more equitable comparison since it also employs BC.

**Flat Baseline:** This non-hierarchical baseline is based on a language-conditioned Decision Transformer (DT) <a href="#reference-10">[10]</a>  and is similar to LISA in implementation but lacks a skill predictor network. The policy here is a Causal Transformer, which uses a numerical representation of the language instructions provided by a pre-trained DistillBERT model <a href="#reference-6">[6]</a>, instead of the future sum of rewards to guide its actions.

## Performance Comparison in Low-Data Regime {#evaluation}

LISA's performance was assessed on three levels from the BabyAI environment (GoToSeq, SynthSeq and BossLevel)  and the LORel Sawyer environment. These levels were chosen because of their complexity and requirement to perform multiple sub-tasks in sequence. The models were trained on randomly sampled subsets of 1k, 10k and 100k trajectories for the BabyAI dataset and 50k trajectories from the LOReL dataset. More data was used for LOReL due to the sub-optimal nature of the trajectories.

The experimental results are presented in the Table below:

<div align="center">
<img src="/hayrulablog.github.io/images/results.png" alt="LISA Training Algorithm" align="center" width="600"/> 
<p style="font-size:14px;">Success rates are shown as percentages. The best method is highlighted in <b>bold</b>. <br> Table taken from <a href="#reference-1">[1]</a></p>
</div>

The results presented in the Table above show that LISA either matches or outperforms the strong non-hierarchical Decision Transformer baselines across all tested environments. As the number of training trajectories decreases, the performance gap between LISA and the flat baseline increases. This means that LISA is more effective at leveraging common sub-task structures and also at extracting more information from limited data. With more training data provided the gap narrows since the flat model encounters more compositions during training, which leads to better generalization.

LISA also achieves twice the performance of the flat Lang-DT baseline on LOReL tasks when using fully observable states. With the partial image observations LISA achieves a 40% success rate, despite the challenging nature of the data. This indicates that LISA's use of discrete skill codes effectively identifies multiple ways of defining the same task, which allows LISA to develop a multi-modal policy. In comparison, the flat model has difficult managing and differentating noisy trajectories, which possibly leads to overfitting and reduced performance.

## Interpreting LISA's Learned Skills
//TO-DO

## Conclusion {#conclusion}
This blog post introduces LISA (Learning Interpretable Skill Abstractions), a framework that enables humans to interact with machines using natural language. LISA successfully understands and executes complex tasks in natural language by learning discrete skill codes through imitation learning. Compared to similar models, LISA performs visibly better in scenarios with limited data, using its hierarchical architecture to handle tasks effectively. 

The conducted experiments show that LISA outperforms a strong non-hierarchical Decision Transformer baseline. In the LOReL Sawyer Environment LISA achieves twice the performance with fully observable states and 40% success rate using partial image observations. This demonstrates LISA's ability to develop a multi-modal policy by successfully identifying different approaches to the same task, in contrast to flat models that struggle with managing noisy data.
## References

<a name="reference-1"></a>
[1] Garg, Divyansh, et al. "LISA: Learning Interpretable Skill Abstractions from Language." Advances in Neural Information Processing Systems, vol. 35, 2022. <a href="https://arxiv.org/abs/2203.00054" target="_blank">LISA paper</a> <br>
<a name="reference-2"></a>
[2] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural computation, 3(1):88â97, (1991). ISSN 0899-7667. <br>
<a name="reference-3"></a>
[3] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals, (2018) <br>
<a name="reference-4"></a>
[4] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning, 2019. <a href="https://arxiv.org/abs/1810.08272" target="_blank">BabyAI paper</a> <br>
<a name="reference-5"></a>
[5] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. <a href="https://arxiv.org/abs/1711.00937">Paper</a><br>
<a name="reference-6"></a>
[6] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020.  <a href="https://arxiv.org/abs/1910.01108" target="_blank">DistilBERT paper</a> <br>
<a name="reference-7"></a>
[7] Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, and Chelsea Finn. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation, 2021. <a href="https://arxiv.org/abs/2109.01115" target="_blank">LOReL paper</a> <br>
<a name="reference-8"></a>
[8]Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale, 2021. <a href="https://arxiv.org/abs/2104.08212" target="_blank">Paper</a><br>
<a name="reference-9"></a>
[9] Divyansh Garg and Skanda Vaidyanath and Kuno Kim and Jiaming Song and Stefano Ermon. LISA: Learning Interpretable Skill Abstractions from Language. (2022) <br>
<a name="reference-10"></a>
[10] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling, 2021. <a href="https://arxiv.org/abs/2106.01345" target="_blank">DT Paper</a><br>
<a name="reference-11"></a>
[11] R.M. Gray and D.L. Neuhoff. Quantization. IEEE Transactions on Information Theory, 44(6):2325â2383, 1998. doi: 10.1109/18.720541. <a href="https://ieeexplore.ieee.org/document/720541">Paper</a>